\renewcommand{\documentname}{Experimental results}

\chapter{Experimental results}\label{experimental-results}

This chapter shows the experimental study carried out to evaluate the quality of the prototype and its algorithms. We advance the main thesis we have arrived at after this study: The greedy algorithm is able to solve the problem without the help of the genetic algorithm. However, it is not powerful enough to find the best solutions obtained with other configurations. Only when coupled with the best version of the genetic algorithm does the software perform at its best.

The experiments were run on a Linux cluster (Intel Xeon 2.26 GHz, 128 GB RAM).


\subsection{Instances}

Four scenarios have been identified for this phase. One scenario is a combination of a group loading level and a constraint loading level. We have groups from the first and second semester of two different academic years, and we have restrictions and preferences obtained from client meetings.

A charge level has two states: regular and charged. Regular groups are simply those found in the academic course planning. Regular constraints are those indicated by the clients. Loaded groups introduce new groups artificially, generating them automatically by code. The same applies to the constraints.

For the charged load level, one group per subject was created for each instance with a 10\% probability, and one restriction or preference with a 20\% probability. Once created, the results were checked for incorrect data or contradictions.

We therefore have the following scenarios:

\begin{itemize}
    \item \textbf{Charged groups - Charged constraints}
    \item \textbf{Charged groups - Regular constraints}
    \item \textbf{Regular groups - Charged constraints}
    \item \textbf{Regular groups - Regular constraints}
\end{itemize}

Each scenario has instances created from the first and second semester of the academic years 20-21 and 21-22.

\subsection{Fitness function}

The fitness function is created based on the client's expectations of results. The highest weight is given to all assignments being performed, followed by the fulfilment of preferences, then the remaining fitness values.

The values for the fitness function follow.

\begin{lstlisting}[basicstyle=\small]
# Fitness weights

# COL_WEIGHT: Weight for the collisions fitness value
COL_WEIGHT = 1.0

# FREE_LABS_WEIGHT: Weight for the free labs fitness value
FREE_LABS_WEIGHT = 0.25

# LANG_WEIGHT: Weight for the group language fitness value
LANG_WEIGHT = 0.25

# SHARED_LABS_WEIGHT: Weight for the shared labs fitness value
SHARED_LABS_WEIGHT = 0.25

# SHARED_THEORY_WEIGHT: Weight for the shared theory classes fitness value
SHARED_THEORY_WEIGHT = 0.25

# PREFS_WEIGHT: Weight for the preferences fitness value
PREFS_WEIGHT = 0.5
\end{lstlisting}



\subsection{Greedy Algorithm}

With this fitness function, we proceed to experiment with the following versions of the greedy algorithm:

\begin{itemize}
    \item \textbf{Base greedy}: Algorithm without repairs and biases.
    \item \textbf{Base greedy + Repairs}: Algorithm with repairs but without biases.
    \item \textbf{Base greedy + Repairs + Biases}: Algorithm with repairs and biases towards preferences and shared classrooms.
\end{itemize}

Experiments are launched ten times per run. Each run executes the greedy algorithm another ten times and returns the best result. An overview of the results will now be given.


Each run stores a number of metrics related to the fitness values. These metrics are listed and defined below.

\begin{itemize}
    \item Average CPU time.
    \item Best fitness.
    \item Average fitness. 
    \item Assignments without classroom. 
    \item Average number of free labs by hour.
    \item Preferences met. 
    \item Fitness which evaluates whether the Spanish and English groups go to different classes.  
    \item Fitness that assesses whether the laboratory groups of a subject go to the same laboratory.
    \item Fitness which measures whether theory groups with the same name and course go to the same classroom.
\end{itemize}


\subsubsection{Fitness assesment}

Although the greedy does not factor in fitness to execute its operations, it can be an indicative value of the quality of a version of the algorithm.

The inclusion of the repair process does not seem to influence the results of the greedy algorithm a lot. However, the biases introduce a substantial improvement, as can be seen in the following summary table of the best fitness values (see Table \ref{table-fn-review}).


\begin{table}[H]
    \centering
    \caption{Greedy algorithm: Best fitness summary}
    \label{table-fn-review}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Version} & \textbf{Best fitness summary} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 149.31 \\
        \rowcolor{blue!10}
        Greedy + Repairs & 149.81 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 166.69 \\
        \hline
    \end{tabular}
\end{table}


\subsubsection{Other metrics}

Evaluating the results, we can see a clear difference between the behaviours of these three versions of the greedy algorithm in terms of assignments and preferences. The algorithms behave similarly in terms of number of unassigned groups (see Tables \ref{table-assign-01} and \ref{table-assign-02}). 


\begin{table}[H]
    \centering
    \caption{Greedy algorithm: Unassigned groups for instance rr\_20\_21\_s1}
    \label{table-assign-01}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\textbf{RegGroups-RegConstraints Course 20-21 Semester 1}} \\
        \hline
        \textbf{Version} & \textbf{Average groups unassigned (out of 340)} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 11 \\
        \rowcolor{blue!10}
        Greedy + Repairs & 9 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 8 \\
        \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \caption{Greedy algorithm: Unassigned groups for instance cr\_21\_22\_s2}
    \label{table-assign-02}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\textbf{CharGroups-RegConstraints Course 21-22 Semester 2}} \\
        \hline
        \textbf{Version} & \textbf{Average groups unassigned (out of 314)} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 6 \\
        \rowcolor{blue!10}
        Greedy + Repairs & 5 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 5 \\
        \hline
    \end{tabular}
\end{table}

However, when we look at the preferences respected by each version, the picture changes. There is a clear advantage of the algorithm with bias over the other two versions. The tables for the previous two instances are now shown except that they now refer to preferences (see Table \ref{table-prefs-01} and \ref{table-prefs-02}).


\begin{table}[H]
    \centering
    \caption{Greedy algorithm: Preferences met for instance rr\_20\_21\_s1}
    \label{table-prefs-01}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\textbf{RegGroups-RegConstraints Course 20-21 Semester 1}} \\
        \hline
        \textbf{Version} & \textbf{Average preferences met (out of 18)} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 1 \\
        \rowcolor{blue!10}
        Greedy + Repairs & 1 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 10 \\
        \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \caption{Greedy algorithm: Preferences met for instance cr\_21\_22\_s2}
    \label{table-prefs-02}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\textbf{CharGroups-RegConstraints Course 21-22 Semester 2}} \\
        \hline
        \textbf{Version} & \textbf{Average preferences met (out of 43)} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 10 \\
        \rowcolor{blue!10}
        Greedy + Repairs & 12 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 17 \\
        \hline
    \end{tabular}
\end{table}

In the shared classroom metrics, an improvement is also observed in the algorithm with bias, since its heuristic contemplates this situation. In the rest of the metrics the difference is not so pronounced


\subsection{(Genetic + Greedy) parameter assesment}

Considering the results of the previous section, the greedy algorithm with repairs and bias is chosen for the genetic algorithm experiments. In this section we will describe the tests performed to tune the GA parameters.

Twenty-seven combinations were considered for the Course 20-21 instances. The values chosen are as follows.

\begin{description}
    \item \textbf{Population size}: $\{ 100, 250, 500 \}$
    \item \textbf{Crossover probability} : $\{ 0.8, 0.9, 1 \}$
    \item \textbf{Mutation probability} : $\{ 0.05, 0.1, 0.2 \}$
\end{description}

The results are summarised in Table \ref{table-combinations}.

\begin{table}[H]
    \centering
    \caption{(Genetic + Greedy): Parameter combinations results (Best fitness)}
    \label{table-combinations}
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \rowcolor{blue!30}
Comb	& rr\_20\_21\_s1	& rr\_20\_21\_s2	& rc\_20\_21\_s1	& rc\_20\_21\_s2	& cr\_20\_21\_s1	& cr\_20\_21\_s2	& cc\_20\_21\_s1	& cc\_20\_21\_s2 \\
        \hline

        Comb01	& 191.58	& 175.19	& 179.98	& 174.46	& 190.14	& 174.1	 & 178.63	& 172.49 \\

        Comb02	& 192.33	& 176.0	& 180.64	& 174.27	& 189.78	& 173.92	& 179.36	& 172.99 \\

        Comb03	& 192.07	& 175.24	& 180.13	& 174.66	& 190.53	& 174.33	& 179.95	& 173.12 \\

        Comb04	& 193.07	& 175.63	& 180.66	& 174.67	& 190.03	& 174.96	& 179.23	& 173.7 \\

        Comb05	& 193.34	& 176.18	& 180.81	& 175.13	& 190.32	& 174.33	& 179.4	& 173.1 \\

        Comb06	& 192.44    & 175.46    & 180.99 	& 175.05	& 190.96	& 174.31	& 179.55	& 173.39 \\

        Comb07	& 193.05	& 176.07	& 181.08	& 174.27	& 190.59	& 174.51	& 179.89	& 174.24 \\

        Comb08	& 193.07	& 176.08	& 181.14	& 175.2	& 190.99	& 175.18	& 180.11	& 174.24 \\

        Comb09	& 192.93	& 176.07	& 180.87	& 174.84	& 190.73	& 174.56	& 179.69	& 174.12 \\

        Comb10	& 191.69	& 174.86	& 179.93	& 174.2	& 190.01	& 173.67	& 179.14	& 172.54 \\

        Comb11	& 192.89	& 174.84	& 180.47	& 175.28	& 190.13	& 174.37	& 179.61	& 173.54 \\

        Comb12	& 192.46	& 175.05	& 180.99	& 174.55	& 190.19	& 174.24	& 179.8	& 172.72 \\

        Comb13	& 192.81	& 175.43	& 181.41	& 174.51	& 190.43	& 173.99	& 180.16	& 173.81 \\

        Comb14	& 193.03	& 176.35	& 180.57	& 174.73	& 190.96	& 174.44	& 179.64	& 173.64 \\

        Comb15	& 192.67	& 175.43	& 180.22	& 174.51	& 191.17	& 175.48	& 179.92	& 174.95 \\

        Comb16	& 193.31	& 175.75	& 180.81	& 175.65	& 190.9	& 174.6	& 179.28	& 174.17 \\
        
        \rowcolor{blue!10}
        Comb17	& 193.19	& 176.18	& 180.47	& 176.19	& 191.14	& 175.08	& 180.39	& 173.85 \\

        Comb18	& 192.9	& 175.88	& 180.78	& 174.96	& 190.83	& 174.91	& 179.87	& 173.99 \\

        Comb19	& 191.79	& 175.06	& 180.65	& 175.07	& 189.48	& 175.37	& 179.44	& 172.87 \\

        Comb20	& 193.41	& 175.5	& 180.65	& 174.8	& 190.26	& 174.62	& 179.83	& 172.98 \\

        Comb21	& 192.89	& 175.35	& 180.14	& 174.08	& 190.55	& 174.23	& 179.89	& 173.02 \\

        Comb22	& 192.47	& 175.65	& 180.31	& 174.98	& 190.68	& 175.04	& 179.91	& 173.68 \\

        Comb23	& 193.2	& 175.9	& 180.87	& 174.54	& 190.98	& 174.39	& 180.21	& 173.45 \\

        Comb24	& 191.95	& 175.54	& 180.95	& 174.31	& 190.18	& 174.13	& 179.72	& 172.93 \\

        Comb25	& 192.38	& 175.64	& 180.83	& 174.85	& 190.68	& 175.96	& 180.05	& 173.84 \\

        Comb26	& 192.86	& 176.26	& 181.85	& 175.44	& 190.91	& 174.69	& 179.46	& 173.32 \\

        Comb27	& 192.73	& 175.55	& 181.08	& 173.75	& 190.22	& 174.83	& 179.76	& 173.16 \\
        \hline
    \end{tabular}
    }
\end{table}


We decided on combination 17 as it had the best fitness results, although there is not much of a difference between the different configurations. The content of this combination are shown below.

\begin{lstlisting}[basicstyle=\small]
POP_SIZE = 250
CROSS_PROB = 0.9
MUTA_PROB = 0.2
\end{lstlisting}



\subsection{Greedy against (Genetic + Greedy)}

We will now use the parameters in the combination 17 for the GA with the initial fitness function, testing the three versions of the greedy algorithm in combination with the genetic algorithm. For this comparison, we will use the previous tables, adding the values obtained with the combination of both algorithms.

We begin by showing the values of the best fitness found (see Table \ref{table-greedgen-fn}).


\begin{table}[H]
    \centering
    \caption{Greedy against (Genetic + Greedy): Best fitness summary}
    \label{table-greedgen-fn}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Version} & \textbf{Best fitness summary} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 149.31 \\
        \rowcolor{blue!10}
        Genetic + Base greedy & 169.50 \\
        \rowcolor{blue!30}
        Greedy + Repairs & 149.81 \\
        \rowcolor{blue!10}
        Genetic + (Greedy + Repairs) & 168.75 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 166.69 \\
        \rowcolor{blue!10}
        Genetic + (Greedy + Rep + Bias) & 179.63 \\
        \hline
    \end{tabular}
\end{table}

Next, the two algorithms will be compared on the other metrics. The four tables in the greedy algorithm evaluation are summarised in two tables (See Tables \ref{table-greedgen-met-01} and \ref{table-greedgen-met-02}).


\begin{table}[H]
    \centering
    \caption{Greedy against (Genetic + Greedy): Metrics for instance rr\_20\_21\_s1}
    \label{table-greedgen-met-01}
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|c|c|c|}
        \hline
        \multicolumn{3}{|c|}{\textbf{RegGroups-RegConstraints Course 20-21 Semester 1}} \\
        \hline
        \textbf{Version} & \textbf{Average groups unassigned (out of 340)} & \textbf{Average preferences met (out of 18)}  \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 11 & 1\\
        \rowcolor{blue!10}
        Genetic + Base greedy & 7 & 7 \\
        \rowcolor{blue!30}
        Greedy + Repairs & 9 & 1 \\
        \rowcolor{blue!10}
        Genetic + (Greedy + Repairs) & 6 & 6 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 8 & 10\\
        \rowcolor{blue!10}
        Genetic + (Greedy + Rep + Bias) & 6 & 13 \\
        \hline
    \end{tabular}
    }
\end{table}


\begin{table}[H]
    \centering
    \caption{Greedy against (Genetic + Greedy): Metrics for instance cr\_21\_22\_s2}
    \label{table-greedgen-met-02}
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|c|c|c|}
        \hline
        \multicolumn{3}{|c|}{\textbf{CharGroups-RegConstraints Course 21-22 Semester 2}} \\
        \hline
        \textbf{Version} & \textbf{Average groups unassigned (out of 314)} & \textbf{Average preferences met (out of 43)} \\
        \hline
        \rowcolor{blue!30}
        Base greedy & 6 & 10\\
        \rowcolor{blue!10}
        Genetic + Base greedy & 4 & 18 \\
        \rowcolor{blue!30}
        Greedy + Repairs & 5 & 12\\
        \rowcolor{blue!10}
        Genetic + (Greedy + Repairs) & 5 & 18 \\
        \rowcolor{blue!30}
        Greedy + Rep + Bias & 5 & 17\\
        \rowcolor{blue!10}
        Genetic + (Greedy + Rep + Bias) & 4 & 18 \\
        \hline
    \end{tabular}
    }
\end{table}

From these tables we can conclude that the greedy algorithm still fares very well when compared to the combination of the greedy and genetic algorithms, but that ultimately the best option is to combine the two.



\subsection{Further fitness functions}

As a final experiment, we decided to perform a comparison between different values of the fitness weights. For this we considered four scenarios:

\begin{itemize}
    \item \textbf{Shared labs and theories}: In this scenario we test whether the genetic algorithm achieves better results by ignoring language separation and focusing on clustering groups in the same classrooms, according to the criteria set out earlier in this document.
    \item \textbf{Group separation by language}: The reverse of the first scenario. Here we focus on separating the groups by language, ignoring whether or not they are grouped in the same classrooms.
    \item \textbf{Equal weights}: All fitness value have a weight of one.
    \item \textbf{Weights for fitness values of collisions and preferences}: Only the unassigned classrooms (collisions) and the preferences get a weight value of 1.0 and 0, respectively. The rest are set to zero.
\end{itemize}

Analysing the results, we can see that the worst configuration is the one where the weights are equal. Granted, it has higher fitness, but in the other metrics it is far inferior to the rest of the configurations.

Naturally, scenario 2 (group separation by language) ends up with results whose language fitness is higher than scenario 1 (shared labs and theories), but lower fitness in shared classes compared to the latter. In terms of assignments without classrooms and fulfilled preferences, both configurations work in a similar way.

If we compare scenarios 1 and 2 with scenario 4, we see that the difference is not so overwhelming as to justify not assigning weights to language or shared class values.



\subsection{Experimental findings}

Reflecting on the previous experiments, one can come to a conclusion that was already anticipated at the beginning of the chapter. This problem is not so complex, therefore, a greedy algorithm with a good heuristic can achieve an acceptable solution in short times.

But, in order to really obtain the best possible solution with these algorithms, it is necessary to use the genetic algorithm with the right combination of parameters and weights.


