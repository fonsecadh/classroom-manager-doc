\renewcommand{\documentname}{Proposed solution}

\chapter{Proposed solution}

We have already studied the theoretical concepts necessary to solve this type of problem, and we have formalised the problem to be solved. In this section we explain which components, techniques and algorithms we have designed and used to solve the problem of assigning classes to groups. To do so, we will define the search space, introduce the concepts of collisions and class filters, and comment on the pseudocode of the proposed algorithms.



\section{Search space}


\subsection{Assignments}

An assignment is a tuple which associates a group with a classroom.

\begin{equation}
    \scalebox{1.5}{$(G_{i}, C_{j})$}
\end{equation}

Because a group can only have \textit{one} classroom assigned, an assignment can be identified by the \textit{code} \footnote{The name convention previously mentioned: \textit{subject.type.name} (e.g. Com.T.1).} of the group. For example, the assignment for group SI.T.1 can be identified by the code SI.T.1 as well.

Assigning just \textit{one} classroom to each group means that the total number of assignments is calculated by the following expression.

\begin{equation}
    \scalebox{1.2}{$TotalNumberOfAssignments = \left|G\right|$}
\end{equation}

This implies that there are as many assignments as the number of groups for the semester.

\subsection{Solutions}

A \textit{solution} for this problem is represented by a set of all the assignments that must be performed for the semester. As presented in the previous section, the total number of assignments equals the total number of groups in that semester. So we have the next statement.

\begin{equation}
    Solution = \{ (G_{1}, C_{x}), (G_{2}, C_{y}), ..., (G_{m}, C_{z})) \}
\end{equation}

Where $m$ is the total number of groups and $x$, $y$ and $z$ are the index for the classrooms assigned to the groups. Note that the classrooms are not sequential (e.g $x$ could represent $C_{12}$ and $y$ represent $C_{3}$).

An \textit{empty solution} is represented by a set of all the assignments where each assignment is \textit{incomplete}. We mean that an assignment is incomplete when the group has no classroom assigned.

\begin{equation}
    IncompleteAssignment = (G_{i}, -)
\end{equation}

So, for the empty solution, we have a set with the following format.

\begin{equation}
    EmptySolution = \{ (G_{1}, -), (G_{2}, -), ..., (G_{m}, -)) \}
\end{equation}

Finally, a \textit{partial solution} is one in which not every assignment was performed, and a \textit{complete solution} is defined by a set in which all the assignments have been performed and each group has a classroom associated with it.

\subsection{States}

A \textit{state} represents a phase in the problem. There can be three states. The \textit{initial state}, which stands for the empty solution of non allocated assignments. The \textit{final state}, which represents a complete solution with all the assignments performed. And the \textit{intermediate states} portraying partial solutions.

A key concept to understand our design is the following. Although by default we start the execution of the algorithms with an initial state, it is also possible to start the execution with an intermediate state. This is because we can receive as input a partial or total solution of assignments and work from there. Now we will discuss how we can jump from one state to the next, which is normally called \textit{state expansion}.

To expand a state, one of the non performed assignments in the solution is executed. This means that every time a classroom is assigned to a group the state is being expanded. To perform an assignment, the number of possible candidates is the same as the number of classrooms. So we have the following.

\begin{equation}
    \scalebox{1.2}{$TotalNumberOfCandidates = \left|C\right|$}
\end{equation}

Nevertheless, as there exist constraints that indicate whether or not the solution is valid, there are filters which reduce the number of available classrooms for a group. This allows for optimized and easy to retrieve calculations in the execution of the greedy algorithm (this will be explained later in \ref{classroom-filters}). The important thing to note at this moment is that, because of these filters, not all states can be expanded.

\subsection{Instances}

The complexity of the calculations and completion time depend on many factors. Some of those factors follow. First, the number of groups for the semester, which directly translates into the number of assignments to be made. Second, the number of classrooms. If there are more classrooms, it is easier to avoid collisions. Obviously, the number of groups is much more volatile between semesters than the number of classes, which is likely to change very occasionally, if at all. Lastly, the case of starting the prototype with an intermediate state. This means that the set of assignments represents a partial solution given as input, and since the number of calculations decreases in direct proportion to the assignments already made, the completion time would be lower.

Because of the constraints of the problem, there are some groups where class allocations are more straightforward. For example, a group with only one positive restriction is either going to have that classroom assigned to it or, if it collides with other group, end up unallocated. This is why all the groups that just have one available classroom are assigned first. Also, the groups which have less students have more available classrooms than those with a large number of members.



\section{Collisions}

A collision is an overlap of the timetable of two different groups. For a collision to occur, the groups must clash at least once in the same week, day and time. Collisions are an essential part of this problem, as we cannot assign a classroom to a group if another group was previously associated with that classroom and both groups collide.

\subsection{Lazy Collision Matrix}\label{lcm}

Due to the large number of assignments that have to be made throughout the execution cycle of the genetic algorithm, the chosen data structures were properly analysed. This is where the \textit{Lazy Collision Matrix} comes in.

Imagine that we have the following group set.

\begin{equation}
    G = \{ G_{1}, G_{2}, G_{3} \}
\end{equation}

Then, our initial Lazy Collision Matrix would be represented by the expression below.

\begin{equation}
    LCM = \bordermatrix{
        & G_{1} & G_{2} & G_{3} \cr
        G_{1} &  & -1 & -1 \cr
        G_{2} & -1 &  & -1 \cr
        G_{3} & -1 & -1 & 
    } \qquad
\end{equation}

First of all, the diagonal is empty because we never compare one group against itself. Then, we can observe that the rest of values are defaulted to $-1$. Why? Because there are \textit{not yet evaluated}. That is the reason behind the name of the matrix. It is \textit{lazy} because the collisions are only calculated when needed.

Continuing with this example, imagine that we assign classroom $C_{x}$ to group $G_{1}$. Then, $G_{2}$ also tries to have $C_{x}$ assigned to it, so we check if both groups collide. We find out that they do, so we update the matrix.

\begin{equation}
    LCM = \bordermatrix{
        & G_{1} & G_{2} & G_{3} \cr
        G_{1} &  & 1 & -1 \cr
        G_{2} & 1 &  & -1 \cr
        G_{3} & -1 & -1 & 
    } \qquad
\end{equation}

Therefore, the values are updated with a $1$, which indicates that both groups \textit{collide}. This results in a different classroom $C_{y}$ being assigned to $G_{2}$. Now, $G_{3}$ has that classroom also available, so we check if it clashes with $G_{2}$. We learn that they do not collide, so we update the matrix again.

\begin{equation}
    LCM = \bordermatrix{
        & G_{1} & G_{2} & G_{3} \cr
        G_{1} &  & 1 & -1 \cr
        G_{2} & 1 &  & 0 \cr
        G_{3} & -1 & 0 & 
    } \qquad
\end{equation}

The value for non-collision is $0$, as observed. Because $G_{3}$ does not clash with $G_{2}$, they are both allocated in the same classroom.

We can now generalise the LCM as in the next expression.

\begin{equation}
    LCM = \bordermatrix{
        & G_{1} & G_{2} & G_{3} \cr
        G_{1} &  & g_{12} & g_{13} \cr
        G_{2} & g_{21} &  & g_{23} \cr
        G_{3} & g_{31} & g_{32} & 
    } \qquad
\end{equation}

Where a value $g_{ij}$ can be

\[
    \begin{cases}
        1\text{,} &\quad\text{if $G_{i}$ collides with $G_{j}$}\\
        0\text{,} &\quad\text{if $G_{i}$ does not collide with $G_{j}$}\\
        -1\text{,} &\quad\text{if the collision has not yet been evaluated}\\
        \epsilon\text{,} &\quad\text{otherwise}
    \end{cases}
\]

The main advantage of this design is that we do not have to calculate all collisions. For example, a collision between a laboratory group and a theory group would be pointless to calculate because they would never be allocated in the same classroom. Therefore we alleviate the number of calculations. 

As there is only one Lazy Collision Matrix, all calculations performed by the greedy algorithm across all populations in all generations are stored in just one place. This means that all collisions are being calculated only when necessary and only once. Think of the previous example. In a future iteration the greedy algorithm wants to check if groups $G_{1}$ and $G_{2}$ collide. It access the corresponding location in the LCM, and because it contains a $1$, the greedy concludes that they indeed collide. This is done with a $O(1)$ complexity, as the matrix is coded as a dictionary of dictionaries. If instead the greedy wanted to check if groups $G_{1}$ and $G_{3}$ collide, because the LCM has a $-1$ in that position, the greedy would have to perform the collision check and then update the matrix.



\section{Classroom filters}\label{classroom-filters}

A classroom filter is a function. It receives as input either the $C$ set or a subset $I \subset C$, and outputs a new subset $F \subset C$ with the available classrooms for a given group. It filters out the classrooms that do not comply with the hard constraints for that particular group (except collisions, which are calculated later with the LCM, refer to \ref{lcm}). All groups have the \textit{same} classroom filters.

For example, let us consider a group $G_{i}$ with type $T_{j}$ and $x$ students. A type filter for $G_{i}$ would remove from the result set all the classrooms with a type different from $T_{j}$. A capacity filter would then take the  set resulting from the previous type filter and use it as input. Then it would eliminate from the set all classrooms with a capacity lower than $x$ and return the new $F$ set.

This reduces the number of classrooms that the greedy has to evaluate and therefore decreases the complexity of the calculations. Furthermore, the filters are deterministic, that is, the execution of the filters of a group will always give the same results. This means that, if needed, the filters are only performed once per group per execution.

This may lead us to this question. \textit{If all classroom filters are executed only once per group every time you run the prototype, why bother using a lazy approach for storing them? Would it not be better to perform and store them in a dictionary at the start of the execution?} The answer is no. It is true that if we start from an empty solution the lazy approach does not present a big advantage. Nonetheless, in the case of partial solutions, it reduces the number of calculations. For example, if we want to assign a classroom to a new group created in the middle of the semester, the only thing we care about is if the group collides with any other group. The filters for the rest of the groups are, in that situation, irrelevant. This is due to the fact that they have already been allocated to their corresponding classrooms.


\subsection{Lazy Filter Dictionary}\label{lfd}

The filters work in a similar way to the Lazy Collision Matrix. Again, the election of the data structures is crucial to optimise the execution times. That is why the classroom filters are coded using a dictionary of sets. Once more, we will explain this with an example.

We have a $M$ set of $n$ classroom filters, three groups $G_{1}$, $G_{2}$ and $G_{3}$, and two classrooms $C_{1}$ and $C_{2}$.

\begin{align}
    M &= \{ M_{1}, M_{2}, ..., M_{n} \}\\
    G &= \{ G_{1}, G_{2}, G_{3} \}\\
    C &= \{ C_{1}, C_{2} \}
\end{align}

Then, we define the dictionary as a function $Dict: Keys \rightarrow Values \cup \{ \epsilon \}$, where $\epsilon$ is the $null$ character. That is, $\epsilon \notin Values$.

The keys are represented by the groups, so $Keys \equiv G$. The values depict the different sets of available classrooms for each group. Because the dictionary is as lazy as the LCM, the calculations are performed as needed, so the initial set of values are by default $\epsilon$. Then we can say that $Values = \{ \epsilon, \epsilon, \epsilon \}$.

Accordingly, we have the following cases.

\[
    Dict(x) =
    \begin{cases}
        \epsilon\text{,} &\quad\text{if} x = G_{1}\\
        \epsilon\text{,} &\quad\text{if} x = G_{2}\\
        \epsilon\text{,} &\quad\text{if} x = G_{3}\\
        \epsilon\text{,} &\quad\text{otherwise}
    \end{cases}
\]

We are now in the first execution of the greedy algorithm. Because we start from an empty state and not from an intermediate step, the greedy will try to assign a classroom for all groups. As a result of the values in the dictionary being $null$, the greedy knows that it must execute the filters, updating the dictionary. This is the LFD after the first execution for this case.

\[
    Dict(x) =
    \begin{cases}
        F_{1}\text{,} &\quad\text{if} x = G_{1}\\
        F_{2}\text{,} &\quad\text{if} x = G_{2}\\
        F_{3}\text{,} &\quad\text{if} x = G_{3}\\
        \epsilon\text{,} &\quad\text{otherwise}
    \end{cases}
\]

With each $F_{i} \subset C$ being the filtered classrooms for each group. Example of values for the $F$ sets follow.

\begin{align}
    F_{1} &= \{ C_{1} \}\\
    F_{2} &= \{ C_{1}, C_{2} \}\\
    F_{3} &= \{ \}
\end{align}

We can observe that the for $G_{1}$, classroom $C_{2}$ was filtered out. In the case of $G_{2}$, both classrooms comply with all constraints. And for $G_{3}$, there are no classes available. This is very important, because it implies that with these filters, $G_{3}$ will \textit{always} end up without a classroom. Therefore, a complete solution cannot be found for this case unless the filters are changed. Lastly, a final remark. We can say that from now on, until the prototype terminates, the greedy will not have to perform the filters for any of the groups again, as the results are already stored in the dictionary. 



\section{Greedy algorithm}

The greedy algorithm (see \ref{theory-greedy} for details) performs the assignments taking care not to infringe any hard constraint. To meet this objective, the LFD and the LCM are used. Its pseudocode is as follows.

\begin{algorithm}[H]
    \caption{ClassManager Greedy Algorithm}
    \begin{algorithmic}[1]
        \Procedure {GreedyAlgorithm}{assignments}
            \State {$solution\gets copy(assignments)$}
            \State {$repairs\gets \{\}$}
            \State {$solution\gets preprocess(solution)$}
            \For {$\ i \text{ from } 0 \text{ to } length(solution) - 1$}
                \If {$\ !isAssigned(A_{i})$}
                    \State {$c\gets bestClassroomFor(A_{i})$}
                    \If {$\ c \neq \epsilon$}
                        \State {$assignClassroomToGroup(c,\ A_{i},\ solution)$}
                        \State {$assignClassroomToSameGroups(c,\ A_{i},\ solution)$}
                    \Else
                        \State {$addToRepairs(A_{i},\ repairs)$}
                    \EndIf
                \EndIf
            \EndFor
            \State {$solution\gets repair(repairs,\ solution)$}
            \State \textbf{return} $solution$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

As can be noted, the algorithm receives a set of empty or partial assignments as an input parameter and returns another set with the assignments made. It iterates over all assignments and calculates those that are not already completed. It does this by obtaining the best class for an assignment, and if found, it assigns it not only to that group, but to all groups to which that group is related. So what does it mean if one group is related to another? It's simple, if the group is a lab group, the classroom is assigned to that group and to all lab groups belonging to its subject. If, on the other hand, the group is a theory group, the classroom found is assigned to that group and to all the theory groups in its course that share the same name.

If no class is found for a group, the group is added to the repair set. Groups belonging to that set are attempted to be fixed at the end of the procedure, before returning the set with the solution.


\subsection{Preprocessing}

The greedy algorithm preprocesses the input set with the assignments, working on those that are not yet complete. The first thing it looks at is whether the set of filtered classes for a given group contains only one element, i.e. whether only one class can be assigned to that group. If this is the case, an attempt is made to make the assignment directly, checking that there are no conflicts.

\begin{algorithm}[H]
    \caption{ClassManager Greedy Algorithm Preprocessing}
    \begin{algorithmic}[1]
        \Procedure {Preprocess}{solution}
            \For {$\ i \text{ from } 0 \text{ to } length(solution) - 1$}
                \If {$\ !isAssigned(A_{i})$}
                    \State {$filtered\gets filterClassroomsFor(group(A_{i}))$}
                    \If {$\ length(filtered) = 1$}
                        \State {$c\gets bestClassroomFor(A_{i})$}
                        \If {$\ c \neq \epsilon$}
                            \State {$assignClassroomToGroup(c,\ A_{i},\ solution)$}
                        \EndIf
                    \EndIf
                \EndIf
            \EndFor
            \State \textbf{return} $solution$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

We can observe that in this procedure the class found is not assigned to the groups related to the one we are evaluating, as it is done in the greedy algorithm itself later on. The reason for this is that additional assignments may cause another group with only one filtered class to remain unassigned, so priority is given to such groups.


\subsection{Heuristic}

The heuristic used by the greedy algorithm is very simple. Classes are filtered by type, capacity and constraints (positive and negative). Once filtered, they are iterated until one is found that does not cause a collision between the evaluated group and the rest of the groups previously assigned to that classroom.

\begin{algorithm}[H]
    \caption{ClassManager Greedy Algorithm Assignment Heuristic}
    \begin{algorithmic}[1]
        \Procedure {BestClassroomFor}{$A_{i}$}
            \State {$selected \gets \epsilon$}
            \State {$filtered \gets filterClassroomsFor(group(A_{i}))$}
            \For {$\ j \text{ from } 0 \text{ to } length(filtered) - 1$}
                \State {$selected \gets F_{j}$} \Comment{Filtered classroom $j$}
                \If {$\ !collisionExistsFor(group(A_{i}),\ selected)$}
                    \State {break out of the filtered classrooms \textit{for loop}}
                \EndIf
                \State {$selected \gets \epsilon$}
            \EndFor
            \State \textbf{return} $selected$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


\subsection{Repairs}

To conclude with the greedy algorithm components, we must talk about the assignment repair process, which is probably the most complex of all components. As previously explained, in the case of not finding an appropriate class for a group, such a group is put into the repair set. The repair process iterates each group in this set and tries to fix its assignment. For this purpose, it obtains the filtered classes of the group and, for each class, it obtains the groups that are in conflict with the evaluated group. If there is only one conflict for a class, the assignment of the conflicting group is removed and the class is assigned to the group under repair. Then, an attempt is made to obtain a new class for the conflicting group. If successful, both end up with an assigned class and the repair succeeds. If unsuccessful, the previous class is reassigned to the conflicting group and the process is repeated for each conflicting group in each class. If, after the execution of this process, the repair is unsuccessful, the group is finally unassigned.

The pseudocode for the repairing procedure follows.

\begin{algorithm}[H]
    \caption{ClassManager Greedy Algorithm Repairing Process}
    \begin{algorithmic}[1]
        \Procedure {Repair}{repairs, solution}
            \For {$\ i \text{ from } 0 \text{ to } length(repairs) - 1$}
                \State {$filtered \gets filterClassroomsFor(group(A_{i}))$}
                \For {$\ j \text{ from } 0 \text{ to } length(filtered) - 1$}
                    \State {$collisions \gets collisionsFor(group(A_{i}),\ F_{j})$}
                    \If {$\ length(collisions) > 1$}
                        \State {break out of the filtered classrooms \textit{for loop}} \Comment{Assignment could not be repaired.}
                    \EndIf
                    \State {$group \gets firstElement(collisions)$}
                    \State {$a \gets assignmentFor(group)$}
                    \State {$removeClassroomFromGroup(F_{j},\ a,\ solution)$}
                    \State {$assignClassroomToGroup(F_{j},\ A_{i},\ solution)$}
                    \State {$c \gets bestClassroomFor(a)$}
                    \If {$\ c \neq \epsilon$}
                        \State {$assignClassroomToGroup(c,\ a,\ solution)$} \Comment{Assignment repaired.}
                    \Else
                        \State {$removeClassroomFromGroup(F_{j},\ A_{i},\ solution)$} \Comment{Assignment could not be repaired.}
                        \State {$assignClassroomToGroup(F_{j},\ a,\ solution)$}
                    \EndIf
                \EndFor
            \EndFor
            \State \textbf{return} $solution$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}



\section{Genetic Algorithm}

The Genetic Algorithm (GA) (see \ref{theory-GA} for details) generates sets of assignments, empty or partial, with different ordering. This sets are later piped into the greedy algorithm, which performs and returns the final assignments to the GA. The GA then evaluates the solution by calculating its fitness value. This is done until a predetermined time or number of generations is reached, returning the best order of assignments, that is, the best individual found.

\begin{algorithm}[H]
    \caption{ClassManager Genetic Algorithm (GA)}
    \begin{algorithmic}[1]
        \Procedure {GeneticAlgorithm}{popsize, numgen, maxTime, crossProb, mutProb}
            \State {$best\gets \epsilon$}
            \State {$currentTime\gets \text{get current time}$}
            \State {$gen\gets 0$}
            \State {$P \gets \{ \}$} 
            \For {$\ popsize \text{ times}$}
                \State {$P\gets P \cup \{ \text{new random individual} \}$}
            \EndFor
            \Repeat
                \State {$evaluate(P)$}\Comment{Calculate the fitness of all individuals.}
                \For {$\ \text{each individual } P_{i} \in P$}
                    \If {$\ best = \epsilon \text{ or } Fitness(P_{i}) > Fitness(best)$}
                        \State {$best\gets P_{i}$}
                    \EndIf
                \EndFor
                \State {$Q\gets nextGeneration(P,\ popsize,\ crossProb,\ mutProb)$}
                \State {$P\gets Q$}
                \State {$currentTime\gets \text{update time}$}
                \State {$gen\gets gen + 1$}
            \Until {$\ gen \geq numgen\ \text{ or } currentTime \geq maxTime$}
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

This pseudocode is very similar to the general GA, except for two things. Here, instead of having a function that checks whether or not the individual represents a valid solution, our GA simply iterates through a number of generations, stopping when it reaches that number or when time runs out. The other thing in which they differ is that the operations carried out for creating the next generation are performed in a different procedure. This procedure selects the parents, checks whether or not the offspring should result from a crossover of its parents \footnote{This check is performed by calculating a random number that is compared with the crossover probability. If the selected number is lower or equal to the probability, the crossover is executed.} and also if the offspring is mutated \footnote{Same check as for the crossover operator but comparing the number with the mutation probability.}. A tournament is then performed between the two pairs of parents and children and the two best, in terms of fitness, are added to the next generation. The pseudocode for this procedure follows.

\begin{algorithm}[H]
    \caption{ClassManager GA Next Generation}
    \label{cm-ga-ng}
    \begin{algorithmic}[1]
        \Procedure {NextGeneration}{$P$, popsize, crossProb, mutProb}
            \State {$Q\gets \{ \}$}
            \For {$\ popsize / 2 \text{ times}$}
                \State {$\text{Parent } P_{a} \gets selectAndRemove(P)$}
                \State {$\text{Parent } P_{b} \gets selectAndRemove(P)$}
                \State {$\text{Child } C_{a} \gets copy(P_{a})$}
                \State {$\text{Child } C_{b} \gets copy(P_{b})$}
                \If {$\ crossProb \geq \text{random number from $0.0$ to $1.0$ inclusive}$}
                    \State {$C_{a} \gets crossover(copy(P_{a}),\ copy(P_{b}))$}
                    \State {$C_{b} \gets crossover(copy(P_{b}),\ copy(P_{a}))$}
                \EndIf
                \If {$\ mutProb \geq \text{random number from $0.0$ to $1.0$ inclusive}$}
                    \State {$C_{a} \gets mutation(copy(C_{a}))$}
                    \State {$C_{b} \gets mutation(copy(C_{b}))$}
                \EndIf
                \State {$\text{Winners } W_{a}, W_{b} \gets tournament(P_{a},\ P_{b},\ C_{a},\ C_{b})$}
                \State {$Q\gets Q \cup \{ W_{a}, W_{b} \}$}
            \EndFor
            \State \textbf{return} $Q$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection{Genome representation}

The individuals of the GA are represented by a vector chromosome of group codes. Each code uniquely identifies a group, therefore an assignment, and the genome tells the greedy algorithm in which order it must perform that assignment. The rationale behind this random ordering is clear if we think back on what we discussed about greedy algorithms. Their main flaw relies on not always finding the correct solution because they focus on local optimum instead of global ones. The GA helps the greedy by generating a bunch of different orderings and therefore testing which order gets closer to the optimal solution.

A vector chromosome looks like this. Let's say that $I$ represents an individual, represented by a set of group codes in a random order. Codes identify assignments, so they are indicated by $A_{i}$.

\begin{equation}
    I = \{ A_{1}, A_{2}, ..., A_{m}\}
\end{equation}

Where $m$ is the number of groups for that semester.

To convert the individual representation into an actual set of assignments in the specified order, we have designed a \textit{decoder}. The decoder has a dictionary given by the function $Dict: Keys \rightarrow Values \cup \{ \epsilon \}$. The group codes work as keys, and the assignments related to each group are the corresponding values. When an individual is decoded, the decoder simply takes each group code, obtains the value associated to them by looking in the dictionary, and then returns a set with the order given by the representation.


\subsection{Fitness function}

The fitness function of the GA has the following responsibilities. First, it passes the individual representation to the decoder in order to obtain the corresponding set of assignments. Then, the function gives the assignments to the greedy algorithm and receives the solution set. Finally, it returns the value resulting from applying its formula to the solution set. 

The formula of the fitness function is given by the sum of all fitness values multiplied by their corresponding fitness weights. Each fitness value represents a soft constraint described in the problem definition (see \ref{problem-definition}), and has a weight associated with it. 

The formula for the fitness function is therefore given by:

\begin{equation}
    formula = \sum_{i=1}^{n} V_{i} W_{i}
\end{equation}

Where $n$ is the number of fitness values defined, $V_{i}$ is fitness value $i$ and $W_{i}$ the weight associated with $V_{i}$.


\subsection{Operators}

We will briefly talk about the selected operators for the ClassManager GA (see \ref{theory-GA} for a more in-depth explanation of all the operators discussed here). The order and use of these operators can be clearly illustrated with the help of the pseudocode of procedure $NextGeneration$ (see \ref{cm-ga-ng}).


\subsubsection{Selection}

The GA uses Random Selection. This operator selects and removes a random individual from the population. Since at the end of the offspring creation cycle there is a tournament between parents and offspring, a random selection of individuals is perfectly valid in this case. The pseudocode of this operator is displayed in \ref{theory-ga-ransel}


\subsubsection{Crossover}

As we explained when we introduced this operator, the Order Crossover (OX) is the crossover operator for the ClassManager GA. As in our case it only returns one child per pair of parents, OX is called two times, with the positions of the parents inverted (check the $NextGeneration$ pseudocode to see what I mean), that is, parent $a$ acts as the first parent in the first run and as the second parent in the second run (and vice versa for parent $b$). See \ref{theory-ga-ox} for its pseudocode.

It is also important to note that the offspring are only created from a crossover if a random number between $0.0$ and $1.0$ is lower or equal to the predetermined crossover probability.


\subsubsection{Mutation}

For the mutation operator the GA uses Swap Mutation. This operator selects two group codes at random and swaps their positions. As in the crossover operator, the mutation only occurs if a randomly selected value is lower or equal to the mutation probability. The pseudocode for Swap Mutation is indicated in \ref{theory-ga-sm}.   


\subsubsection{Tournament}

Finally, we introduce the specific version of Tournament Selection implemented for the ClassManager GA. It differs from the generic Tournament Selection (see \ref{theory-ga-ts}) in that the candidates for the tournament are not a $t$ number of randomly selected individuals, but instead the two pairs of parents and children generated after executing the rest of operators. These four individuals are compared by their fitness and the best two survive, joining the new generation. Below we indicate the pseudocode for this version of Tournament Selection.

\begin{algorithm}[H]
    \caption{ClassManager GA Tournament Selection}
    \begin{algorithmic}[1]
        \Procedure {TournamentSelection}{A, B, C, D}
            \State {$I \gets \{ \}$}
            \State {$I \gets I \cup \{ A,\ B,\ C,\ D \}$}
            \State {sort $I$ by fitness, from highest to lowest}
            \State \textbf{return} $I_{1} \text{ and } I_{2}$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection{Parameters}

TODO: AFTER EXPERIMENTS

