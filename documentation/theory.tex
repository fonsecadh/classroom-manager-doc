\renewcommand{\documentname}{Theoretical Aspects}

\chapter{Theoretical aspects}\label{theory}

A digital magazine Bootaku works with three freelancers. Dante, Virgil and Beatrice. Together they write a section about book reviews. Gathering data from previous sections, Bootaku wants to define and solve a problem of efficiently assign all reviews to the three critics so that the section gets the highest profit. For the assignments, Bootaku wants every book review to have one (and only one) associated freelancer. If a freelancer ends up with no reviews, the assignments are still valid if and only if the previous condition is met.


\section{Assignment problem}

The problem described before is an example of an assignment problem. It can be generalised with the following elements: 

\begin{description}
    \item A set of $n$ freelancers $f$
    \item A set of $m$ book reviews $r$
    \item An assignment matrix of $n \times m$ assignments $a_{fr}$ such that $a_{fr} = 0$ when freelancer $f$ is not assigned to book review $r$ and $a_{fr} = 1$ when freelancer $f$ is assigned to book review $r$.
    \item A profit matrix of $n \times m$ profits $p_{fr}$ which indicate the profit obtained when assigning freelancer $f$ to book review $r$ and that $p_{fr} \textgreater 0$.
    \item A valid solution is defined as a matrix of assignments where all the book reviews have a freelancer assigned to them and no book review has more than one associated freelancer.
\end{description}

The profit for all the assignments will then be:

\begin{equation}
    \scalebox{2}{$\sum_{f=1}^n \sum_{r=1}^m a_{fr} p_{fr}$}
\end{equation}

The optimal solution consists on having a set of assignments such that the sum of all the profits for the current assignments is maximised.

For example, imagine that for the next month's section, we have the following data. The information is represented by means of two sets: $F$ for the freelancers and $R$ for the reviews.

\begin{align}
    F &= \{ Dante, Virgil, Beatrice \} \\
    R &= \{ {Divina\ Commedia}, {El\ Quijote}, {Voyage\ au\ bout\ de\ la\ nuit}, {Todo\ modo} \}
\end{align}

Then, our assignments and profits will be represented by the $A$ and $P$ matrices.

\begin{equation}
    A = \bordermatrix{
        & DC & EQ & VN & TM \cr
        Dante & a_{11} & a_{12} & a_{13} & a_{14} \cr
        Virgil & a_{21} & a_{22} & a_{23} & a_{24} \cr
        Beatrice & a_{31} & a_{32} & a_{33} & a_{34} 
    } \qquad
\end{equation}

\begin{equation}
    P = \bordermatrix{
        & DC & EQ & VN & TM \cr
        Dante & p_{11} & p_{12} & p_{13} & p_{14}\cr 
        Virgil & p_{21} & p_{22} & p_{23} & p_{24}\cr 
        Beatrice & p_{31} & p_{32} & p_{33} & p_{34} 
    } \qquad
\end{equation}

Where each row represents a freelancer and each column represents a book review. So freelancer 1 is Dante, 2 is Virgil and 3 is Beatrice. The same goes for the book reviews. Book review 1 is \textit{Divina Commedia}, 2 is \textit{El Quijote}, 3 is \textit{Voyage Au Bout De La Nuit} and 4 is \textit{Todo Modo}.

Now, we are going to study valid and non-valid solutions. As we explained before, a solution is valid if every book review has a freelancer assigned to it, and no more than one.

We will analyse four sets of values for the A matrix:

\begin{align}
    A1 &= 
    \begin{bmatrix}
        1 & 0 & 0 & 0\\ 
        0 & 0 & 1 & 0\\ 
        0 & 1 & 0 & 1
    \end{bmatrix} \\
    A2 &= 
    \begin{bmatrix}
        0 & 0 & 0 & 0\\ 
        1 & 0 & 1 & 0\\ 
        0 & 1 & 0 & 1 
    \end{bmatrix} \\
    A3 &= 
    \begin{bmatrix}
        0 & 0 & 1 & 0\\ 
        0 & 0 & 0 & 1\\ 
        1 & 0 & 0 & 0 
    \end{bmatrix} \\
    A4 &= 
    \begin{bmatrix}
        0 & 1 & 1 & 0\\ 
        0 & 0 & 0 & 1\\ 
        1 & 1 & 0 & 0 
    \end{bmatrix}
\end{align}

From these matrices, we can deduce that $A1$ and $A2$ are valid solutions, because they have one freelancer for each book review. We are not concerned with a freelancer having no book reviews assigned. However, a book review without an associated freelancer represents a non-valid solution. That is precisely the case for $A3$, the book review for \textit{El Quijote} has not an assigned freelancer. In the case of $A4$, the fact that \textit{El Quijote} has two freelancers assigned makes it a non-valid solution.

Now, we will give values to the $P$ matrix in order to discuss possible optimal solutions. We will compare them with the assignment matrices $A1$ and $A2$

\begin{align}
    P1 &= 
    \begin{bmatrix}
        9 & 1 & 5 & 4\\ 
        2 & 8 & 14 & 2\\ 
        7 & 11 & 10 & 6
    \end{bmatrix} \\
    P2 &= 
    \begin{bmatrix}
        9 & 1 & 5 & 4\\ 
        13 & 8 & 14 & 2\\ 
        7 & 11 & 10 & 6
    \end{bmatrix}
\end{align}

$P1$ and $A1$:

\begin{equation}
    \scalebox{1.2}{$Profit = \sum_{f=1}^n \sum_{r=1}^m a_{fr} p_{fr} = 9 + 11 + 14 + 6 = 40$}
\end{equation}

$P1$ and $A2$:

\begin{equation}
    \scalebox{1.2}{$Profit = \sum_{f=1}^n \sum_{r=1}^m a_{fr} p_{fr} = 2 + 11 + 14 + 6 = 33$}
\end{equation}

We can observe that for $P1$, the assignments defined in $A1$ are better than those in $A2$, because they result in a better profit. Another important remark about $A1$ is that it is the optimal solution to the problem, because it assigns the book reviews to the freelancers with the best profit value for their assigned books. Now $P2$ will be evaluated.

$P2$ and $A1$:

\begin{equation}
    \scalebox{1.2}{$Profit = \sum_{f=1}^n \sum_{r=1}^m a_{fr} p_{fr} = 9 + 11 + 14 + 6  = 40$}
\end{equation}

$P2$ and $A2$:

\begin{equation}
    \scalebox{1.2}{$Profit = \sum_{f=1}^n \sum_{r=1}^m a_{fr} p_{fr} = 13 + 11 + 14 + 6 = 44$}
\end{equation}

In the case of the profits values of $P2$, the situation is reversed. $A2$ is now the optimal solution and therefore better than $A1$.

The important thing to notice here is that the values for the $P$ matrix right now may appear as having no meaning whatsoever. But we need to think of $P$ as the results obtained from a profit funtion. Then, we can interpret $P1$ as values of profit in a context where Virgil has just expressed an opinion on social media about the \textit{Divina Commedia} and caused a massive controversy. We can then say that $P1$ is a function which gives more importance to public relations and so the profit $p_{21}$ is very low, whereas $P2$ gives more importance to views and so the profit $p_{21}$ is higher. Of course, in a real problem you know what the function is calculating, but this shows how we can add meaning to a set of symbols in order to understand the data more efficiently.

With this, we have discussed an assignment problem, looked at its main components and analysed its non-valid, valid and optimal solutions. Some final remarks about the relation between a general assignment problem and the Bootaku problem follow. The actors that perform the jobs, in this case the freelancers that \textit{write} the reviews, are called the \textit{agents}. The \textit{tasks} to be performed are, in the Bootaku problem, the book reviews. Nevertheless, the agents in an assignment problem do not need to be persons (or even things that carry out actions), they can be machines, warehouses, or classrooms. The same can be said for the tasks.


\section{Greedy algorithms}\label{theory-greedy}

One way of solving the Bootaku problem described earlier can be found in \textit{greedy algorithms}. A greedy algorithm \cite{guerequetavallecillo98algdesign} will try to find a subset of candidates that meet the problem constraints and that form the optimal solution. To do so, the algoritm is run iteratively. In each iteration, it will select the best candidate for that precise moment, neglecting future consequences (that is why they are called \textit{greedy})\footnote{For example, let's say I'm walking down the street and I get thirsty. On my mental list of candidate drinks, water has a value of 5 points, lemonade 3 and tea 1. The first vending machine I come across on the street only sells lemonade and tea, so I buy lemonade. However, on the next street I find another vending machine that sells water, but as I am no longer thirsty I don't buy any more drinks. That is, I found a valid solution but not the optimal solution.}. Before adding a candidate to the solution, the algorithm will determine if it is promising. If the answer is yes, then the candidate is added to the solution. Otherwise, the candidate is no longer evaluated. Each time a candidate is added to the solution, the greedy checks whether the current solution is valid or not.

With this in mind, here follows the \textit{pseudocode} of the generic Greedy Algorithm.

\begin{algorithm}[H]
    \caption{Generic Greedy Algorithm}
    \begin{algorithmic}[1]
        \Procedure {GreedyAlgorithm}{candidates}
            \State {$x \gets \epsilon$} 
            \State {$solution\gets \{\}$}
            \State {$found\gets false$}
            \While {$\ !isEmpty(candidates)\ \text{ and } !found$}
                \State {$x\gets selectCandidate(candidates)$}
                \If {$\ isPromising(x,\ candidates)$}
                    \State {$addToSolution(x,\ solution)$}
                    \If {$\ isSolution(solution)$}
                        \State {$found\gets true$}
                    \EndIf
                \EndIf
            \EndWhile
            \State \textbf{return} $solution$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

As we can see, the generic greedy algorithm has a very simple and elegant definition. However, even though greedy algorithms are easy to implement and can obtain efficient solutions, they are not perfect. Their main flaw relies on their  selection function. It is difficult to design a function that can simultaneously find a good local result and translate it into a good global result. That is, the best candidate in some iteration may not be part of the optimal solution. 

Next, we will solve the Bootaku problem using a greedy algorithm. The greedy algorithm that we are going to use is inspired by the one defined in \cite{guerequetavallecillo98algdesign} for solving the \textit{Assignment of tasks} problem.

\begin{algorithm}[H]
    \caption{Bootaku Greedy Algorithm}
    \begin{algorithmic}[1]
        \Procedure {GreedyBootaku}{profits, assignments}
            \State {$best \gets \epsilon$}
            \For {$\ \text{each freelancer } F_{i} \in F$}
                \For {$\ \text{each review } R_{j} \in R$}
                    \State {$assignments[F_{i},\ R_{j}] = false$} \Comment{We initialise the assignments matrix (to false or cero, it does not matter).}
                \EndFor
            \EndFor
            \For {$\ \text{each review } R_{j} \in R$}
                \State {$best \gets bestFreelancerFor(profits,\ assignments,\ R_{j})$}
                \State {$assignments[best,\ R_{j}] = true$} \Comment{Again, it can be true or one, depending on the implementation.}
            \EndFor
            \State \textbf{return} $assignments$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Bootaku Greedy Algorithm BestFreelancerFor}
    \begin{algorithmic}[1]
        \Procedure {BestFreelancerFor}{profits, assignments, review}
            \State {$best \gets \epsilon$} 
            \State {$min\gets \text{maximum integer value}$}
            \For {$\ \text{each freelancer } F_{i} \in F$}
                \If {$\ profits[F_{i},\ review] < min$}
                    \State {$min \gets profits[F_{i},\ review]$}
                    \State {$best \gets F_{i}$}
                \EndIf
            \EndFor
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Those are the two procedures needed in order to solve the Bootaku problem. In the \textit{Assignments of tasks} problem described in the book, the authors define an extra function that checks if the worker is already assigned to another task. However, because our problem is not balanced (we have a different number of tasks and agents), it means that we can have a freelancer writing more than one book review, so that extra function is not required.

The Bootaku problem is really simple to solve because of its lack of constraints. This is done deliberately to focus more on the components of assignment problems and not to waste time on explaining difficult restrictions. However, most problems, including the real problem this document defines (to assign classrooms to the groups of the School), have a lot of constraints. 

One way to complicate the Bootaku problem would be to assign completion times to each review. We would have a $n \times m$ $T$ matrix with the completion times for all freelancers and reviews.

\begin{equation}
    T = \bordermatrix{
        & DC & EQ & VN & TM \cr
        Dante & t_{11} & t_{12} & t_{13} & t_{14} \cr
        Virgil & t_{21} & t_{22} & t_{23} & t_{24} \cr
        Beatrice & t_{31} & t_{32} & t_{33} & t_{34} 
    } \qquad
\end{equation}

Then we could have a maximum time per freelancer. This would force the greedy algorithm to perform a check before assigning a review to a freelancer. If the time it takes to write the review surpases the maximum time available for that freelancer, the assignment cannot be made. 

Let's say that Beatrice has been assigned to the book review for \textit{El Quijote}, so she has already spent a total time of $t_{32} \leq maxTime$. In a future iteration the greedy algorithm evaluates Beatrice for reviewing \textit{Todo Modo}. Even if she has the greatest profit for \textit{Todo modo}, it is still not enough. The greedy algorithm first has to check in the $BestFreelancerFor$ procedure if $t_{32} + t_{34} \leq maxTime$ and, if the condition is true, then the assignment is performed. 

We can notice in the Beatrice example the main failure of greedy algorithms. She was assigned to \textit{El Quijote} for a profit $p_{32}$ and then evaluated again for \textit{Todo modo} with a profit $p_{34}$. Imagine that she has not enough time left to be able to review the second book and that $p_{34}$ is \textit{way bigger} than $p_{32}$. This is where assigning the best local result $a_{32}$ would end up ruling out the possibility of assigning the better global result $a_{34}$. Later in this chapter we will see a possible way of fixing this problem with the help of genetic algorithms, but before that we will conduct a study on \textit{metaheuristics}.



\section{Heuristics and metaheuristics}

Search strategies in a search problem can be \textit{informed} or \textit{uninformed}. Informed strategies use knowledge specific to a given problem but that is outside of its definition, making this type of strategies more efficient than uninformed strategies. The main way of applying our knowledge of a given problem into the search algorithm designed to solve said problem is by means of \textit{heuristic functions}. An heuristic function $h(n)$ \cite{russellnorvig10ai} represents an estimation of the minimum cost of getting to the objective state from the state given by node $n$. To expand a node, the algorithm also makes use of an \textit{evaluation function}. An evaluation function $f(n)$ analyses the non-expanded nodes and selects the one with the lowest cost. In the case of greedy algorithms, the evaluation function of a node n is equivalent to the heuristic function of the same node. So we have that $f(n) = h(n)$.

Now that we have explained what heuristic functions are, one question remains. What are \textit{metaheuristics}? Analysing the word, one could think that the \textit{meta} prefix implies that metaheuristics are \textit{heuristics about heuristics}, in the same way \textit{metadata} is \textit{data about data}. However, as Luke \cite{luke13metaheuristics} points out, this is not the case at all. He defines metaheuristics as:

\begin{displayquote}
    ... a rather unfortunate term often used to describe a major subfield, indeed the primary subfield, of \textbf{stochastic optimization}. Stochastic optimization is the general class of algorithms and techniques which employ some degree of randomness to find optimal (or as optimal as possible) solutions to hard problems. Metaheuristics are the most general of these kinds of algorithms, and are applied to a very wide range of problems.
\end{displayquote}

There are many methods of designing algorithms based on \textit{metaheuristics}. In this project we will focus on the Evolutionary Computation method, a subtype of Population-based methods.


\subsection{Evolutionary Computation}

Evolutionary Computation (EC) \cite{luke13metaheuristics} takes inspiration from population biology, genetics and evolution \footnote{Because this method uses vocabulary from these fields of biology, we have followed Luke's approach and defined these terms one by one. A list of definitions of the most commonly used terms in EC has been created in the annex of definitions and abbreviations.}. We are interested in the types of algorithms designed using this method, known as Evolutionary Algorithms (EAs). An EA may be (usually) either a \textit{generational algorithm} or a \textit{steady-state algorithm}.  A generational algorithm creates a new population of individuals, based on the previous one, in each iteration. Moreover, a steady-state algorithm changes a subset of individuals in each iteration, but not the entire population. The most common EAs are the \textit{Genetic Algorithms} and the \textit{Evolution Strategies}, and there are generational and steady-state versions of the two.

Below is the pseudocode for an abstract generational algorithm.

\begin{algorithm}[H]
    \caption{Abstract Generational Algorithm}
    \begin{algorithmic}[1]
        \Procedure {AbstractGenerationalAlgorithm}{maxTime}
            \State {$P \gets \text{create the initial population}$} 
            \State {$best\gets \epsilon$}
            \State {$currentTime\gets \text{get current time}$}
            \While {$\ !idealSolution(P)\ \text{ and } currentTime \leq maxTime$}
                \State {$evaluate(P)$}\Comment{Calculate the fitness of all individuals.}
                \For {$\ \text{each individual } P_{i} \in P$}
                    \If {$\ best = \epsilon \text{ or } Fitness(P_{i}) > Fitness(best)$}
                        \State {$best\gets P_{i}$}
                    \EndIf
                \EndFor
                \State {$P\gets newGeneration(P,\ breed(P))$}
                \State {$currentTime\gets \text{update time}$}
            \EndWhile
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The initial population in this kinds of algorithms is created by adding random individuals to a set until the maximum population size is reached. Some good practices for this process follow. The most important thing is not generating repeated individuals. This can be done with a dictionary in which we store the individuals as keys. For every new randomly generated individual we check if it is not already contained in said dictionary before adding it to the population set. Finally, it is possible to include individuals designed by hand into the initial population (this is called \textit{seeding} the population). However, the use of EAs already implies that finding a good heuristic for the problem is not trivial. So even if we think that the individuals we design may be going in the right direction, it is very likely that they will end up producing poor results.

The main difference between generational EAs relies on how they create the new generation. This process is done by means of different operations such as \textit{selection}, \textit{crossover} and \textit{mutation}. Also, some EAs simply discard all the parents in the new generation and others include them again if they have an acceptable fitness. We will take a look at two specific EAs in the following sections, Evolution Strategies and Genetic Algorithms.


\subsection{Evolution Strategies}

Evolution Strategies (ES) \cite{luke13metaheuristics} are a type of Evolutionary Algorithms. They make use of a selection operator called \textit{Truncation Selection}. This operator consists of selecting individuals from the highest to the lowest fitness value until a predetermined number of selected candidates is reached. For creating the new generation, ES simply use the mutation operator, without combining it with a crossover operator.

The only ES to be covered in this section is the $(\mu,\ \lambda)$ algorithm. We have chosen it because it is one of the simplest ES and therefore easier to understand. The design of $(\mu,\ \lambda)$ is essentially one version of the Abstract Generational Algorithm that details the way in which the new generation is built. This new generation is constructed by using both $\mu$ and $\lambda$ parameters.

In this algorithm, we have an initial population of $\lambda$ number of individuals which are randomly generated. Then, we evaluate the fitness of all individuals, as we did in the Abstract Generational Algorithm, and we calculate the best individual in the generation. The next step is to create the new generation. To do so, $(\mu,\ \lambda)$ performs the Truncation Selection on the parents, selecting the $\mu$ number of parents with greatest fitness, and for each parent a $\lambda / \mu$ number of children are generated. A mutation operation is performed to create the offspring from a copy of their parent. This whole process is done until we arrive at an optimal solution or the maximum time runs out. The pseudocode of the $(\mu,\ \lambda)$ algorithm is shown as follows.

\begin{algorithm}[H]
    \caption{The $(\mu,\ \lambda)$ Evolution Strategy}
    \begin{algorithmic}[1]
        \Procedure {MuLambdaES}{$\mu$, $\lambda$, maxTime}
            \State {$best\gets \epsilon$}
            \State {$currentTime\gets \text{get current time}$}
            \State {$P \gets \{ \}$} 
            \For {$\ \lambda \text{ times}$}
                \State {$P\gets P \cup \{ \text{new random individual} \}$}
            \EndFor
            \While {$\ !idealSolution(P)\ \text{ and } currentTime \leq maxTime$}
                \State {$evaluate(P)$}\Comment{Calculate the fitness of all individuals.}
                \For {$\ \text{each individual } P_{i} \in P$}
                    \If {$\ best = \epsilon \text{ or } Fitness(P_{i}) > Fitness(best)$}
                        \State {$best\gets P_{i}$}
                    \EndIf
                \EndFor
                \State {$Q\gets \mu \text{ individuals with the highest to lowest fitness}$}
                \State {$P \gets \{ \}$} 
                \For {$\ \text{each individual } Q_{j} \in Q$}
                    \For {$\ \lambda / \mu \text{ times}$}
                        \State {$P\gets P \cup \{ mutation(copy(Q_{j})) \}$}
                    \EndFor
                \EndFor
                \State {$currentTime\gets \text{update time}$}
            \EndWhile
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Knowing how to give values to the parameters $\lambda$, $\mu$ and the mutation probability is very important in this algorithm. In the case of $\lambda$, as it approaches $\infty$ the algorithm starts behaving as a random search algorithm, so it is best if it does not have an excessively high value. For $\mu$, if we give it a very low value, the algorithm becomes very selective and focuses only of a specific type of individual with a high fitness value. This may result in \textit{premature convergence} of the algorithm and thus end the execution with a locally optimal rather than a globally optimal solution. Lastly, because the mutation operation determines the similarity between parents and offsprings, if the mutation probability is very high, the new population would be very different from the previous one. Therefore, it would make children resemble random individuals.

To conclude the section, simply note that there is another algorithm, very similar to this one, called $(\mu + \lambda)$. The only difference between $(\mu + \lambda)$ and $(\mu,\ \lambda)$ is that while $(\mu,\ \lambda)$ discards the parents when creating the new generation, $(\mu + \lambda)$ makes a union between parents and children. This makes each new generation of the $(\mu + \lambda)$ algorithm having a size of $\mu + \lambda$, where $\mu$ is the number of parents and $\lambda$ the number of new offspring. Because very fit parents can survive for several generations, $(\mu + \lambda)$ behaves like a $(\mu,\ \lambda)$ with a very low $\mu$ number, it can terminate with a premature convergence.


\subsection{Genetic algorithms} \label{theory-GA}

The Genetic Algorithm (GA) \cite{luke13metaheuristics} is a type of Evolutionary Algorithms with a strong similarity towards the $(\mu,\ \lambda)$ Evolution Stategy. What separates the two algorithms the most is the selection operation and the way a new generation is created. In $(\mu,\ \lambda)$, candidates were chosen by Truncated Selection. Once the candidates are selected, the next generation is populated by mutations of the parental copies. However, in the GA, a pair of parents is selected and their offspring are immediately created, adding them to the new generation. This process of simultaneous selection and reproduction occurs until the population reaches the maximum number of individuals set. Further discussion of the GA will follow, once we have seen its pseudocode.

\begin{algorithm}[H]
    \caption{The Genetic Algorithm (GA)}
    \begin{algorithmic}[1]
        \Procedure {GeneticAlgorithm}{popsize, maxTime}
            \State {$best\gets \epsilon$}
            \State {$currentTime\gets \text{get current time}$}
            \State {$P \gets \{ \}$} 
            \For {$\ popsize \text{ times}$}
                \State {$P\gets P \cup \{ \text{new random individual} \}$}
            \EndFor
            \While {$\ !idealSolution(P)\ \text{ and } currentTime \leq maxTime$}
                \State {$evaluate(P)$}\Comment{Calculate the fitness of all individuals.}
                \For {$\ \text{each individual } P_{i} \in P$}
                    \If {$\ best = \epsilon \text{ or } Fitness(P_{i}) > Fitness(best)$}
                        \State {$best\gets P_{i}$}
                    \EndIf
                \EndFor
                \State {$Q\gets \{ \}$} \Comment{Here the GA begins to differ from $(\mu,\ \lambda)$.}
                \For {$\ popsize / 2 \text{ times}$}
                    \State {$\text{Parent } P_{a} \gets selectWithReplacement(P)$}
                    \State {$\text{Parent } P_{b} \gets selectWithReplacement(P)$}
                    \State {$\text{Children } C_{a}, C_{b} \gets crossover(copy(P_{a}), copy(P_{b}))$}
                    \State {$Q\gets Q \cup \{ mutate(C_{a}), mutate(C_{b}) \}$}
                \EndFor
                \State {$P\gets Q$}
                \State {$currentTime\gets \text{update time}$}
            \EndWhile
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

As we said before, the GA is differentiated from the $(\mu,\ \lambda)$ ES by means of its selection, crossover and mutation operators. We will now elaborate on these operators, defining them and explaining some of their implementations.

\subsubsection{Selection}

We begin with the selection operator. Even if the selection variant in the GA is different from the one in $(\mu,\ \lambda)$, the concept of selection is equivalent in both algorithms. In other words, both use the operation of selection to obtain the parents who will produce the children of the future generation.. There can be multiple ways to implement the selection operator, including \textit{Random Selection}, \textit{Fitness-Proportionate Selection}, \textit{Stochastic Universal Sampling}, \textit{Tournament Selection} and a variant of the GA which includes \textit{elitism}.

The Random Selection variant, as its name implies, picks two random parents, removing them from the population. After generating the offspring, another pair of parents is selected and so on, until the new population is complete. In the GA designed to solve the problem of classroom management, we used this variant combined with a tournament between the randomly selected pair of parents and their two generated children to select the best two individuals out of the four (in terms of fitness).

\begin{algorithm}[H]
    \caption{Random Selection}
    \label{theory-ga-ransel}
    \begin{algorithmic}[1]
        \Procedure {RandomSelection}{P}
            \State {$selected \gets \text{random individual from population $P$}$} 
            \State {$remove(selected,\ P)$} \Comment{The individual is removed from the population.}
            \State \textbf{return} $selected$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Next, we will address the topic of Fitness-Proportionate Selection, also known as \textit{Roulette Selection}. In this variant, all individuals are dimensioned according to their fitness. If we think of this process as a lottery, the larger the size of an individual, i.e. the greater the fitness, the more likely the individual is to win the lottery prize. In this case that prize is to be selected to be a parent. This means that a random number $n$ such that $0 \leq n \leq \text{\textit{the total sum of all fitness values}}$ will fall in range of one of the individuals, thus selecting such an individual.

\begin{algorithm}[H]
    \caption{Fitness-Proportionate Selection}
    \begin{algorithmic}[1]
        \Procedure {GenerationPreparations}{P}\Comment{Executed only one time at the start of each generation.}
            \State {$\textbf{global}\ \vec{p} \gets \langle p_{1},p_{2},...,p_{l} \rangle$} \Comment{Vector which copies all individuals in P.}
            \State {$\textbf{global}\ \vec{f} \gets  \langle f_{1},f_{2},...,p_{l} \rangle$} \Comment{Vector with the fitness values of all the individuals in $\vec{p}$, keeping the order.}
            \If {$\ \vec{f} \text{ contains only zeros}$}
                \State {Substitute all items of $\vec{f}$ with ones}
            \EndIf
            \For {$\ i \text{ from } 2 \text{ to } l$} \Comment{Convert $\vec{f}$ into a cumulative distribution.}
                \State {$f_{i}\gets f_{i} + f_{i-1}$}
            \EndFor
        \EndProcedure
        \Procedure {FitnessProportionateSelection}{P}
            \State {$n\gets \text{random number from $0$ to $f_{i}$ inclusive}$} 
            \State {$selected \gets p_{1}$}
            \For {$\ i \text{ from } 2 \text{ to } l$}
                \If {$\ f_{i-1} < n \leq f_{i}$}
                    \State {$selected \gets p_{i}$}
                \EndIf
            \EndFor
            \State \textbf{return} $selected$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

We can observe that this operator uses two procedures. The first one is an auxiliary procedure named \textit{GenerationPreparations}, which defines the global vector variables $\vec{p}$ and $\vec{f}$ representing the individuals of the population and their fitness values. The second and main one is the actual selection. In this main procedure we obtain a random number and extract the individual whose range of values contains that chosen random number.

A derivative of the Fitness-Proportionate Selection operator mentioned at the beginning of the section is called Stochastic Universal Sampling (SUS). SUS has two very similar procedures to the ones shown before, one executed one time each generation (normally), which defines some global variables and a main procedure which performs the selection. The main difference comes in how the selection is performed. Let $s$ be the sum of all fitness values and $l$ be the population size. A random number generated between $0$ and $s / l$ will select the individual in that range. For every remaining selection the position value, which started in the random number, is increased by $s / l$ and a new selection is carried out (up to a maximum of $l$ times).

\begin{algorithm}[H]
    \caption{Stochastic Universal Sampling Selection}
    \begin{algorithmic}[1]
        \Procedure {GenerationPreparations}{P}\Comment{Executed only one time at the start of each generation.}
            \State {$\textbf{global}\ \vec{p} \gets \langle p_{1},p_{2},...,p_{l} \rangle$} \Comment{Vector which copies all individuals in P.}
            \State {$\vec{p} \gets shuffle(\vec{p})$}
            \State {$\textbf{global}\ \vec{f} \gets \langle f_{1},f_{2},...,p_{l} \rangle$} \Comment{Vector with the fitness values of all the individuals in $\vec{l}$, keeping the order.}
            \State {$\textbf{global}\ index \gets 0$}
            \If {$\ \vec{f} \text{ contains only zeros}$}
                \State {Substitute all items of $\vec{f}$ with ones}
            \EndIf
            \For {$\ i \text{ from } 2 \text{ to } l$} \Comment{Convert $\vec{f}$ into a cumulative distribution.}
                \State {$f_{i}\gets f_{i} + f_{i-1}$}
            \EndFor
            \State {$\textbf{global}\ value \gets \text{random number from $0$ to $f_{l}/l$ inclusive}$}
        \EndProcedure
        \Procedure {SUS}{P}
            \While {$\ f_{index} < value$}
                \State {$index \gets index + 1$}
            \EndWhile
            \State {$value \gets value + f_{l}/l$}
            \State \textbf{return} $P_{index}$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The main advantage of Stochastic Universal Sampling over Fitness-Proportionate selection lies in the fact that while an individual with a high fitness (higher than $s/l$) might never be chosen in a Fitness-Proportionate selection, in the Stochastic Universal Sampling variant its selection is guaranteed.

The last variant we will explain is the Tournament Selection. In contrast with these past two variants, the Tournament Selection is a very straightforward algorithm. In it, a $t$ number of candidates are selected and the fittest is returned, like a sports competition. Every time a $t_{i}$ candidate is selected, it is removed from the population. The pseudocode for this variant is shown below. 

\begin{algorithm}[H]
    \caption{Tournament Selection}
    \label{theory-ga-ts}
    \begin{algorithmic}[1]
        \Procedure {TournamentSelection}{P, t}
            \State {$best \gets \text{random individual from population $P$}$}
            \State {$remove(best,\ P)$}
            \For {$\ i \text{ from } 2 \text{ to } t$}
                \State {$next \gets \text{random individual from population $P$}$}
                \State {$remove(next,\ P)$}
                \If {$\ fitness(next) > fitness(best)$}
                    \State {$best \gets next$}
                \EndIf
            \EndFor
            \State \textbf{return} $best$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

This variant is both simple and flexible. Its flexibily comes from the variable size of candidates for the tournaments. Some considerations for the value of $t$ follow. If $t$ is very low, the operator behaves like a random search. However, if $t$ is very high, the individual with the greatest fitness value will have a much higher likelihood of showing up and getting picked every time. As we stated when talking about Random Selection, a combination of both Random and Tournament selections were implemented in the final design of the GA used in the prototype.

To conclude, we will explain the concept of elitism in the AG. Elitism simply takes a predetermined number of individuals from the previous generation sorted by fitness and includes them directly in the next one before spawning offspring. This decreases the number of offspring created to maintain the fixed population number. A GA with elitism is therefore similar to the $(\mu + \lambda)$ ES and can terminate with premature convergence if not set up correctly. 


\subsubsection{Crossover}

The crossover operator mixes the genomes of a pair of parents to produce new children. We saw how in $(\mu,\ \lambda)$ the offsprings of the $\mu$ selected parents were created by mutating copies of their parents, without ever using the crossover operator. This is not the case for the GA, in this algorithm the mutation occurs after the genome of a child is created from the mixture of the genome of its parents. There are several variants of this operator. The following alternatives are briefly outlined in this section: \textit{One-Point Crossover}, \textit{Two-Point Crossover}, \textit{Uniform Crossover} and \textit{Order Crossover}.

The One-Point and Two-Point crossover work in a similar fashion. They choose random numbers and swap sections of the genomes of the parents to create the genomes of their offspring. We will give an example before showing the pseudocode of both variants.

We have the following chromosomes, representing the parents.

\begin{align}
    P_{a} &= \color{red} \{ 1, 5, 3, 6, 2, 7, 4 \}\\
    P_{b} &= \color{blue} \{ 2, 5, 7, 3, 4, 6, 1 \}
\end{align}

One-Point picks a value at random from $0$ to $6$ and the outcome is $4$. The position at $4 - 1$ (the number selected marks the end of the section and is excluded from it) is the point in which the crossover between parents is produced. The operator then generates these two children.

\begin{align}
    C_{a} &= \color{red}{ \{ 1, 5, 3, 6,\ } \color{blue}{2, 7, 4} \}\\
    C_{b} &= \color{blue}{ \{ 2, 5, 7, 3,\ } \color{red}{4, 6, 1} \}
\end{align}

In the case of Two-Point, two values are randomly selected, once again from $0$ to $6$. These values indicate the ends of the section that both parents will exchange, the first being the initial position included in the section and the second being the final position excluded from the section. The selected numbers are $2$ and $4$, which leads to the generation of the following offspring.

\begin{align}
    C_{a} &= \color{red}{ \{ 1, 5,\ } \color{blue}{3, 6,\ } \color{red}{2, 7, 4} \}\\
    C_{b} &= \color{blue}{ \{ 2, 5,\ } \color{red}{7, 3,\ } \color{blue}{4, 6, 1} \}
\end{align}

The pseudocode for these two variants is presented below.

\begin{algorithm}[H]
    \caption{One-Point Crossover}
    \begin{algorithmic}[1]
        \Procedure {OnePoint}{$P_{a}$, $P_{b}$, popsize}
            \State {$x \gets \text{random integer from $0$ to $popsize - 1$}$} 
            \State {$C_{a} \gets copy(P_{a})$} 
            \State {$C_{b} \gets copy(P_{b})$} 
            \If {$\ x \neq 0 $}
                \For {$\ i \text{ from } 0 \text{ to } x - 1$}
                    \State {Swap values of $C_{ai}$ and $C_{bi}$}
                \EndFor
            \EndIf
            \State \textbf{return} {$C_{a}$ and $C_{b}$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Two-Point Crossover}
    \begin{algorithmic}[1]
        \Procedure {TwoPoint}{$P_{a}$, $P_{b}$, popsize}
            \State {$x \gets \text{random integer from $0$ to $popsize - 1$}$} 
            \State {$y \gets \text{random integer from $0$ to $popsize - 1$}$} 
            \State {$C_{a} \gets copy(P_{a})$} 
            \State {$C_{b} \gets copy(P_{b})$} 
            \If {$\ x > y$}
                \State {Swap $x$ and $y$}
            \EndIf
            \If {$\ x \neq y $}
                \For {$\ i \text{ from } x \text{ to } y - 1$}
                    \State {Swap values of $C_{ai}$ and $C_{bi}$}
                \EndFor
            \EndIf
            \State \textbf{return} {$C_{a}$ and $C_{b}$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The main problem with both algorithms is that it is common to break the \textit{linkage} (or \textit{epistasis}) between the elements in the chromosome \cite{luke13metaheuristics}. Imagine that a pair of elements of an individual produces a high fitness with certain element values. This pair is considerably separated on the chromosome, so it is most likely that it will be split when executing the parental crossover. This implies that a section of the chromosome that could give a good fitness value is broken in two, and each half is given to the offspring produced by the crossover, which could overall produce a worse fitness for the new individuals. In this respect, Two-Point is better than One-Point, but still faces the same problem. 

To reduce linkage breaks in the chromosome we can take a look at the Uniform Crossover variant. In this algorithm, all the elements of the chromosome are iterated and, for each one, a random choice of a number from 0.0 to 1.0 is made. If the number chosen is less or equal to a previously defined probability, the parent elements at that position are swapped. The pseudocode of the Uniform Crossover is provided below.

\begin{algorithm}[H]
    \caption{Uniform Crossover}
    \begin{algorithmic}[1]
        \Procedure {UniformCrossover}{$P_{a}$, $P_{b}$, popsize, probSwap}
            \State {$C_{a} \gets copy(P_{a})$} 
            \State {$C_{b} \gets copy(P_{b})$} 
            \For {$\ i \text{ from } 0 \text{ to } popsize - 1$}
                \If {$\ probSwap \geq \text{ random number from $0.0$ to $1.0$ inclusive }$}
                    \State {Swap values of $C_{ai}$ and $C_{bi}$}
                \EndIf
            \EndFor
            \State \textbf{return} {$C_{a}$ and $C_{b}$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Finally, we will explain the operator used in the designed algorithm for the classroom management problem, the Order Crossover (OX) \cite{davis85ox}. The OX operator was first introduced in 1985 at a key Artificial Intelligence conference in Los Angeles, California \cite{joshi85ai}. In this algorithm, two random numbers are picked, similarly to the Two-Point Crossover. Then, the selected section from the first parent is copied onto the offspring, keeping order and position (from point $p_{1}$ to $p_{2}-1$). It is now that OX and Two-Point differ. In Two-Point, all but the replaced section is kept the same in the offspring. Yet, in OX, the elements of the second parent which are still not present in the offspring are copied to it \textit{in the same order as they appear}, from the end to the start of the section (in a circular way, from point $p_{2}$ to $p_{1}-1$). We will illustrate this algorithm with some examples and then provide its pseudocode. 

We will use the same parents as before. Imagine that the random numbers (from $0$ to $6$) result in $p_{1} = 2$ (included in the section) and $p_{2} = 6$ (excluded from the section). We then have the following.

\begin{align}
    P_{a} &= \color{black}{ \{ 1, 5,\ } \color{red}{3, 6, 2, 7,\ } \color{black}{4} \}\\
    P_{b} &= \color{black}{\{} \color{red}{2,\ } \color{black}{5,\ } \color{red}{7, 3,\ } \color{black}{4,\ } \color{red}{6,\ } \color{black}{1} \}
\end{align}

Therefore, the offspring generated by this pair of parents is as described below.

\begin{equation}
    C = \color{black}{\{ 4, 1,\ } \color{red}{3, 6, 2, 7,\ } \color{black}{5 \}}
\end{equation}

To give another example, let's say the numbers chosen are $p_{1} = 5$ and $p_{2} = 2$. The result is as follows.

\begin{align}
    P_{a} &= \color{black}{\{} \color{red}{1, 5,\ } \color{black}{3, 6, 2,\ } \color{red}{7, 4} \color{black}{\}}\\
    P_{b} &= \color{black}{\{ 2,\ } \color{red}{5, 7,\ } \color{black}{3,\ } \color{red}{4,\ } \color{black}{6,\ } \color{red}{1} \color{black}{\}}\\
    C &= \color{black}{\{} \color{red}{1, 5,\ } \color{black}{2, 3, 6,\ } \color{red}{7, 4} \color{black}{\}}
\end{align}

Displayed below is the pseudocode for the OX.

\begin{algorithm}[H]
    \caption{Order Crossover (OX)}
    \label{theory-ga-ox}
    \begin{algorithmic}[1]
        \Procedure {OX}{$F$, $S$, len} \Comment{$F$ and $S$ represent first and second parents, $len$ is the individual length}
            \State {$p_{1} \gets \text{random integer from $0$ to $popsize - 1$}$} 
            \State {$p_{2} \gets \text{random integer from $0$ to $popsize - 1$}$} 
            \State {$C \gets copy(F)$} \Comment{First parent is copied to the offspring.}
            \State {$overflow \gets 0$}
            \If {$\ p_{2} \leq p_{1}$}
                \State {$overflow \gets len$}
            \EndIf
            \State {$k \gets p_{2}$} \Comment{$k =$ Offspring position pointer.}
            \For {$\ i \text{ from } 0 \text{ to } len - 1$} \Comment{$i =$ Second parent position pointer}
                \State {$j \gets p_{1}$} \Comment{$j =$ First parent position pointer.}
                \While {$\ j < p_{2} + overflow \text{ and } S_{i} \neq F_{j \mod len}$}
                    \State {$j \gets j + 1$} \Comment{Iterate section $p_{1} \text{ to } p_{2}-1$}
                \EndWhile
                \If {$\ j = p_{2} + overflow $} \Comment{If $S_{i}$ is not in the section}
                    \State {$C_{k \mod len} \gets S_{i}$}
                    \State {$k \gets k + 1$}
                \EndIf
            \EndFor
            \State \textbf{return} $C$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


\subsubsection{Mutation}

We end these sections on operators by discussing the mutation operator in the GA. Mutation takes an individual and modifies its genome. This can be done in many ways. We will talk about two, \textit{Bit-Flip Mutation} and swapping elements.

Bit-Flip mutation can only work with genomes represented by boolean values (0 and 1). This operator works in a somewhat comparable fashion to the Uniform Crossover operator. We define a flip probability and select a random number from $0.0$ to $1.0$. If the number is lower or equals to the flip probability, we flip the boolean value in that position of the chromosome.

\begin{algorithm}[H]
    \caption{Bit-Flip Mutation}
    \begin{algorithmic}[1]
        \Procedure {BitFlip}{$C$, popsize, probSwap}
            \For {$\ i \text{ from } 0 \text{ to } popsize - 1$}
                \If {$\ probSwap \geq \text{ random number from $0.0$ to $1.0$ inclusive }$}
                    \State {$C_{i} \gets invertValue(C_{i})$}
                \EndIf
            \EndFor
            \State \textbf{return} $C$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Swap mutation consists of taking a pair of values and exchanging their positions. The advantage of this variant over Bit-Flip is that Swap Mutation can be used with chromosomes consisting of non-boolean values. 

\begin{algorithm}[H]
    \caption{Swap Mutation}
    \label{theory-ga-sm}
    \begin{algorithmic}[1]
        \Procedure {SwapMutation}{$C$, popsize}
            \State {$x\gets \text{random number from $0$ to $popsize - 1$}$}
            \State {$y\gets \text{random number from $0$ to $popsize - 1$}$}
            \While {$x = y$}
                \State {$y\gets \text{random number from $0$ to $popsize - 1$}$}
            \EndWhile
            \State {Swap positions of $C_{x}$ and $C_{y}$}
            \State \textbf{return} $C$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}



\section{Mixing it all together}

In this chapter we have discussed assignment problems, greedy algorithms, heuristics, metaheuristics and evolutionary computation. We studied the Bootaku problem and gave a solution with a greedy algorithm. However, although we have mentioned several times in the document that the actual algorithm developed in this thesis contains a genetic algorithm guided by a greedy algorithm, we have not gone into the specifics of what this combination involves.

In this section we will solve the Bootaku problem with the completion times extension explained in \ref{theory-greedy}. First of all, we need to define a chromosome representation for the Bootaku problem. Because the tasks of the problem are the book reviews, the chromosome is represented by the book codes, in this case each code consists of two initials to uniquely identify a book review.

Some possible genomes for the problem will have the following structure.

\begin{align}
    Individual\ A &= \{ DC, EQ, VN, TM \}\\
    Individual\ B &= \{ VN, EQ, DC, TM \}\\
    Individual\ C &= \{ TM, VN, EQ, DC \}
\end{align}

This genomes represent \textit{the order in which the book reviews will be assigned}. As each freelancer can only write a number of book reviews that do not exceed a maximum completion time, the order in which these reviews are assigned is a very important factor to consider if we want to obtain the optimal solution. The use of different orders helps to overcome the main problem of greedy algorithms, which is the difficulty of converting local optimum to global optimum.

Next, we choose the operators which the Genetic Algorithm will use. We decide that we want a Tournament Selection, an OX for the crossover operator and a Swap Mutation. The initial generation is created by generating random individuals and verifying every time that they are unique, before adding them to the population. The function for evaluating fitness will execute the Greedy Algorithm with the assignments performed in the order specified by the chromosome. It will then calculate the total sum of the profits obtained, which will indicate the fitness of the individual. Finally, the parameters of the genetic algorithm will be calculated by experimenting with different problem instances and checking that the assignments made are of the expected quality.

In conclusion, we can say that the Bootaku problem helps us to understand, on a smaller scale, a similar (but simpler) problem that we face in this project, that of assigning classes to groups in the school. In the following sections, we will replace the Bootaku problem with the real problem, building on what we have discussed throughout this chapter.


